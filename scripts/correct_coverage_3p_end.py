"""
Script for adjusting bedgraph file for multi-position coverage rows
"""

import logging
import argparse
import multiprocessing
import sys
import time

import pandas as pd
import numpy as np

__author__ = "Dominik Burri"
__date__ = "2020-11-26"
__license__ = "Apache"
__version__ = "0.0.1"
__maintainer__ = "Dominik Burri"
__email__ = "dominik.burri@unibas.ch"
__status__ = "Development"

def extend_rows(row_tuple):
    'Take row tuple from df.iterrows() and create df of new rows with split coverage'
    new_rows = pd.DataFrame()
    row = row_tuple[1]
    new_row = row.copy()
    for j in range(row.end - row.start):
        new_row.start = row.start + j
        new_row.end = row.start + j + 1
        new_rows = pd.concat([new_rows, new_row.to_frame().T], ignore_index = True)
    return new_rows

#### main program
def main():
    parser = argparse.ArgumentParser(description="Correct 3p end coverage generated by bedtools for cumulative coverage computation.")
    parser.add_argument('-i', dest='file', 
      required=True, help='Input file with 3p end coverage in bedgraph format')
    parser.add_argument('-o', dest='out_file', 
      required=True, help='Output file')
    parser.add_argument('-c', '--cores', dest='NUM_CORES',
      required=False, default=1, type=int,
      help='Specify number of cores for parallelisation. Default: 1.')
    parser.add_argument('-log', '--log-file', dest="log_file", 
      required=False, help='Logging file. Default: correct_coverage_3p_end.log.', default='correct_coverage_3p_end.log')
    args = parser.parse_args()

    # logging settings: from snakemake
    logging.basicConfig(filename=args.log_file, level=logging.INFO, 
                    format='%(asctime)s %(levelname)s:%(message)s', datefmt='%Y-%m-%d %H:%M:%S ')
    
    # load sample files
    sample = pd.read_csv(args.file,
                sep="\t", header=None, 
                names = ["chrom", "start", "end", "coverage"],
                dtype={0: str, 1: np.int64, 2: np.int64, 3: np.int64})
    logging.info("Loaded: " + args.file)
    # get regions with coverage > 0 and longer than 1 bp
    df = sample.loc[(sample.coverage != 0) & (sample.end - sample.start > 1)]
    # create new rows
    try:
        pool = multiprocessing.Pool(args.NUM_CORES)
        logging.info("Started multiprocessing pool with %s cores." % (args.NUM_CORES))
        new_rows = pd.concat(pool.imap(extend_rows, df.iterrows(), chunksize = 250), ignore_index = True)
    except Exception as e:
        sys.exit(logging.error("Something went wrong during multiprocessing: " + str(e)))
        pool.terminate()
        sys.exit(1)
    finally:
        pool.close()
        pool.join()
        logging.info("multiprocessing.Pool closed.")
    # replace old rows
    sample.drop(df.index, inplace = True)
    # append new rows
    sample = sample.append(new_rows, ignore_index = True)
    # sort
    sample.sort_values(by = ['chrom', 'start'], inplace = True)
    # write out
    sample.to_csv(args.out_file, sep = "\t", index = False, header = False)
    logging.info("Finished script.")


if __name__ == "__main__":
    main()